{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fc5316",
   "metadata": {},
   "source": [
    "# Lesson 4 - Checking for hallucinations using NLI\n",
    "\n",
    "Start by setting up the notebook to minimize warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f81ddd-0883-49a5-aa90-0986d2e1e23d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948338f",
   "metadata": {},
   "source": [
    "Import OpenAI client and helpers to set up RAG chatbot and vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3636513-637a-43ae-95eb-351001b41970",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import RAGChatWidget, SimpleVectorDB\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc13a1a",
   "metadata": {},
   "source": [
    "Set up the client, vector database, and system message for the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b36be4b-14ac-41c0-8b9a-b9fef09c2058",
   "metadata": {
    "height": 402
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904b6632a77344c18d51c2b518ff216b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed74896f2e254dfe9270769e325bbbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6535a33de74a81aa7d27e0dc5a38ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1f6ff0d8724a158517693cf820409f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73be60b71eb149e5b39897ffd5fb9e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc6bbc125cc4b0793e2fef4189eff7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbc6a7c6d5a4d89987ddd083482f861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a944b2b873914a0599ebe3cf74597749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a8668d5f5049b7b003b5f7b8f08945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf85bf371f064b238091ba1e1f52aacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dc17d3adfc438fb3fbe97539fb2372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup an OpenAI client\n",
    "unguarded_client = OpenAI()\n",
    "\n",
    "# Load up our documents that make up the knowledge base\n",
    "vector_db = SimpleVectorDB.from_files(\"shared_data/\")\n",
    "\n",
    "# Setup system message\n",
    "system_message = \"\"\"You are a customer support chatbot for Alfredo's Pizza Cafe. Your responses should be based solely on the provided information.\n",
    "\n",
    "Here are your instructions:\n",
    "\n",
    "### Role and Behavior\n",
    "- You are a friendly and helpful customer support representative for Alfredo's Pizza Cafe.\n",
    "- Only answer questions related to Alfredo's Pizza Cafe's menu, account management on the website, delivery times, and other directly relevant topics.\n",
    "- Do not discuss other pizza chains or restaurants.\n",
    "- Do not answer questions about topics unrelated to Alfredo's Pizza Cafe or its services.\n",
    "\n",
    "### Knowledge Limitations:\n",
    "- Only use information provided in the knowledge base above.\n",
    "- If a question cannot be answered using the information in the knowledge base, politely state that you don't have that information and offer to connect the user with a human representative.\n",
    "- Do not make up or infer information that is not explicitly stated in the knowledge base.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca39cea",
   "metadata": {},
   "source": [
    "Initialize the chatbot using the settings above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e4e178-377c-4fcc-8bfb-aa23965f155f",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Setup RAG chatbot\n",
    "rag_chatbot = RAGChatWidget(\n",
    "    client=unguarded_client,\n",
    "    system_message=system_message,\n",
    "    vector_db=vector_db,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdee09a",
   "metadata": {},
   "source": [
    "To revisit the hallucination example from Lesson 1, run the cell below to open the chatbot then paste in the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e64092d-a022-470b-b869-1ffd77a8c1e5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff8e04d8c2f45e3bc14379131726e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(layout=Layout(max_height='300px')), HBox(children=(Textarea(value='', continuous_update=Faâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_chatbot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b7a5fd",
   "metadata": {
    "height": 96
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhow do i reproduce your veggie supreme pizza on my own? can you share detailed instructions?\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy and paste this prompt into the chatbot above:\n",
    "\"\"\"\n",
    "how do i reproduce your veggie supreme pizza on my own? can you share detailed instructions?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152d6f1-8a9a-48f6-9adf-493b71d2f445",
   "metadata": {},
   "source": [
    "## Setup an Natural Language Inference (NLI) Model\n",
    "\n",
    "Import some additional packages to setup the NLI model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4053c667-a9ca-4a8e-a9f4-b827211bffb9",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "# Type hints\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Standard ML libraries\n",
    "import numpy as np\n",
    "\n",
    "# Guardrails imports\n",
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.validator_base import (\n",
    "    FailResult,\n",
    "    PassResult,\n",
    "    ValidationResult,\n",
    "    Validator,\n",
    "    register_validator,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee9ad6",
   "metadata": {},
   "source": [
    "Create a hugging face pipeline to access the NLI model (**Note:** the weights will take about 30 seconds to download):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ad2c92-4114-43bb-9924-a0f3faeb9ce1",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15a6a3a1c4841b8a568a3fb98a14748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/934 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff7814d0f1c499883f2b11b41c3a48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc5043ba98245bba660aed3d248acd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8869f1e32bbb44dd9d047e414e2c9f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce792c878b04282a8822c720d252b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f5e67294444a8287e94b8e3d4e5ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13ec19510da4deab4c4f9888085ab74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entailment_model = \"GuardrailsAI/finetuned_nli_provenance\"\n",
    "NLI_PIPELINE = pipeline(\"text-classification\", model=entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf62d63",
   "metadata": {},
   "source": [
    "Try out the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b36e7f-694e-44ce-8791-66f50a5377c4",
   "metadata": {
    "height": 113
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of an entailed sentence:\n",
      "\tPremise: The sun rises in the east and sets in the west.\n",
      "\tHypothesis: The sun rises in the east.\n",
      "\tResult: {'label': 'entailment', 'score': 0.8697448372840881}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Entailed sentence\n",
    "premise = \"The sun rises in the east and sets in the west.\"\n",
    "hypothesis = \"The sun rises in the east.\"\n",
    "result = NLI_PIPELINE({\"text\": premise, \"text_pair\": hypothesis})\n",
    "print(\n",
    "    f\"Example of an entailed sentence:\\n\\tPremise: {premise}\\n\\tHypothesis: {hypothesis}\\n\\tResult: {result}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1301457-8d22-4c6c-ad10-8d0373ab7577",
   "metadata": {
    "height": 113
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a contradictory sentence:\n",
      "\tPremise: The sun rises in the east and sets in the west.\n",
      "\tHypothesis: The sun rises in the west.\n",
      "\tResult: {'label': 'contradiction', 'score': 0.8648266196250916}\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Contradictory sentence\n",
    "premise = \"The sun rises in the east and sets in the west.\"\n",
    "hypothesis = \"The sun rises in the west.\"\n",
    "result = NLI_PIPELINE({\"text\": premise, \"text_pair\": hypothesis})\n",
    "print(\n",
    "    f\"Example of a contradictory sentence:\\n\\tPremise: {premise}\\n\\tHypothesis: {hypothesis}\\n\\tResult: {result}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0f162-a743-4722-8bac-89e80a725a8d",
   "metadata": {},
   "source": [
    "## Building a Hallucination Validator\n",
    "\n",
    "In this section, you'll build a validator to test for hallucinations in the responses of your RAG chatbot. The validator will check that the response is grounded in the texts of your vector database.\n",
    "\n",
    "Start by setting up a validator with stubs for the `__init__` and `validate` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d562d5-9d14-4507-b89d-1458374dea18",
   "metadata": {
    "height": 181
   },
   "outputs": [],
   "source": [
    "@register_validator(name=\"hallucination_detector\", data_type=\"string\")\n",
    "class HallucinationValidation(Validator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def validate(\n",
    "        self, value: str, metadata: Optional[Dict[str, str]] = None\n",
    "    ) -> ValidationResult:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235cf1f",
   "metadata": {},
   "source": [
    "Next, start fleshing out the pieces of the validator. Start by building the function that will split the response of the LLM into individual sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e110685-a0be-48a2-bda7-b04f84d25527",
   "metadata": {
    "height": 368
   },
   "outputs": [],
   "source": [
    "@register_validator(name=\"hallucination_detector\", data_type=\"string\")\n",
    "class HallucinationValidation(Validator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def validate(\n",
    "        self, value: str, metadata: Optional[Dict[str, str]] = None\n",
    "    ) -> ValidationResult:\n",
    "        # Split the text into sentences\n",
    "        sentences = self.split_sentences(value)\n",
    "        pass\n",
    "\n",
    "    def split_sentences(self, text: str) -> List[str]:\n",
    "        if nltk is None:\n",
    "            raise ImportError(\n",
    "                \"This validator requires the `nltk` package. \"\n",
    "                \"Install it with `pip install nltk`, and try again.\"\n",
    "            )\n",
    "\n",
    "        return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7defb",
   "metadata": {},
   "source": [
    "Now finalize the logic of the validate function. You'll loop through each sentence and check if it is grounded in the texts in the vector database using the `find_relevant_sources` and `check_entailment` functions. Then update the `__init__` function to set up the needed class variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f43e4f8-968b-497b-b93e-20c3e3946090",
   "metadata": {
    "height": 1388
   },
   "outputs": [],
   "source": [
    "@register_validator(name=\"hallucination_detector\", data_type=\"string\")\n",
    "class HallucinationValidation(Validator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: Optional[str] = None,\n",
    "        entailment_model: Optional[str] = None,\n",
    "        sources: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if embedding_model is None:\n",
    "            embedding_model = \"all-MiniLM-L6-v2\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "\n",
    "        self.sources = sources\n",
    "\n",
    "        if entailment_model is None:\n",
    "            entailment_model = \"GuardrailsAI/finetuned_nli_provenance\"\n",
    "        self.nli_pipeline = pipeline(\"text-classification\", model=entailment_model)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def validate(\n",
    "        self, value: str, metadata: Optional[Dict[str, str]] = None\n",
    "    ) -> ValidationResult:\n",
    "        # Split the text into sentences\n",
    "        sentences = self.split_sentences(value)\n",
    "\n",
    "        # Find the relevant sources for each sentence\n",
    "        relevant_sources = self.find_relevant_sources(sentences, self.sources)\n",
    "\n",
    "        entailed_sentences = []\n",
    "        hallucinated_sentences = []\n",
    "        for sentence in sentences:\n",
    "            # Check if the sentence is entailed by the sources\n",
    "            is_entailed = self.check_entailment(sentence, relevant_sources)\n",
    "            if not is_entailed:\n",
    "                hallucinated_sentences.append(sentence)\n",
    "            else:\n",
    "                entailed_sentences.append(sentence)\n",
    "\n",
    "        if len(hallucinated_sentences) > 0:\n",
    "            return FailResult(\n",
    "                error_message=f\"The following sentences are hallucinated: {hallucinated_sentences}\",\n",
    "            )\n",
    "\n",
    "        return PassResult()\n",
    "\n",
    "    def split_sentences(self, text: str) -> List[str]:\n",
    "        if nltk is None:\n",
    "            raise ImportError(\n",
    "                \"This validator requires the `nltk` package. \"\n",
    "                \"Install it with `pip install nltk`, and try again.\"\n",
    "            )\n",
    "        return nltk.sent_tokenize(text)\n",
    "\n",
    "    def find_relevant_sources(self, sentences: str, sources: List[str]) -> List[str]:\n",
    "        source_embeds = self.embedding_model.encode(sources)\n",
    "        sentence_embeds = self.embedding_model.encode(sentences)\n",
    "\n",
    "        relevant_sources = []\n",
    "\n",
    "        for sentence_idx in range(len(sentences)):\n",
    "            # Find the cosine similarity between the sentence and the sources\n",
    "            sentence_embed = sentence_embeds[sentence_idx, :].reshape(1, -1)\n",
    "            cos_similarities = np.sum(\n",
    "                np.multiply(source_embeds, sentence_embed), axis=1\n",
    "            )\n",
    "            # Find the top 5 sources that are most relevant to the sentence that have a cosine similarity greater than 0.8\n",
    "            top_sources = np.argsort(cos_similarities)[::-1][:5]\n",
    "            top_sources = [i for i in top_sources if cos_similarities[i] > 0.8]\n",
    "\n",
    "            # Return the sources that are most relevant to the sentence\n",
    "            relevant_sources.extend([sources[i] for i in top_sources])\n",
    "\n",
    "        return relevant_sources\n",
    "\n",
    "    def check_entailment(self, sentence: str, sources: List[str]) -> bool:\n",
    "        for source in sources:\n",
    "            output = self.nli_pipeline({\"text\": source, \"text_pair\": sentence})\n",
    "            if output[\"label\"] == \"entailment\":\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916852f",
   "metadata": {},
   "source": [
    "Try out the validator. First you'll create an instance of the `HallucinationValidation` class above, passing in the same sentence as you used in the pipeline test above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29175dd8",
   "metadata": {
    "height": 79
   },
   "outputs": [],
   "source": [
    "hallucination_validator = HallucinationValidation(\n",
    "    sources=[\"The sun rises in the east and sets in the west\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922f690",
   "metadata": {},
   "source": [
    "Then use the `validate()` function of this object, passing in the sentence you want to test. The first example does not entail, but the second does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9125a11a",
   "metadata": {
    "height": 96
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation outcome: fail\n",
      "Error message: The following sentences are hallucinated: ['The sun sets in the east']\n"
     ]
    }
   ],
   "source": [
    "result = hallucination_validator.validate(\"The sun sets in the east\")\n",
    "print(f\"Validation outcome: {result.outcome}\")\n",
    "if result.outcome == \"fail\":\n",
    "    print(f\"Error message: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e6ffd57",
   "metadata": {
    "height": 96
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation outcome: pass\n"
     ]
    }
   ],
   "source": [
    "result = hallucination_validator.validate(\"The sun sets in the west\")\n",
    "print(f\"Validation outcome: {result.outcome}\")\n",
    "if result.outcome == \"fail\":\n",
    "    print(f\"Error message: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5978013",
   "metadata": {},
   "source": [
    "In the next lesson, you'll build a guard around this validator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f140f13-caea-4445-913f-06864d7104d4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
